{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Training the model**"
      ],
      "metadata": {
        "id": "nvYQK9kZjVDw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIjHDwDW7r8v",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "fJcDyRrV8UL8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_entities(text, entities):\n",
        "    cleaned_entities = []\n",
        "    seen_tokens = set()\n",
        "    for start, end, label in entities:\n",
        "        entity_text = text[start:end].strip()\n",
        "        start = text.find(entity_text, start, end)\n",
        "        end = start + len(entity_text)\n",
        "        entity_tokens = set(range(start, end))\n",
        "\n",
        "        if not entity_tokens & seen_tokens:\n",
        "            cleaned_entities.append((start, end, label))\n",
        "            seen_tokens.update(entity_tokens)\n",
        "    return cleaned_entities\n"
      ],
      "metadata": {
        "id": "BOWynB5XBhDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
        "    try:\n",
        "        training_data = []\n",
        "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
        "            data = json.load(f)  # Load entire JSON content\n",
        "\n",
        "        for item in data:\n",
        "            text = item['content']\n",
        "            entities = []\n",
        "            if 'annotation' in item and item['annotation'] is not None:\n",
        "                for annotation in item['annotation']:\n",
        "                    point = annotation['points'][0]\n",
        "                    labels = annotation['label']\n",
        "                    if not isinstance(labels, list):\n",
        "                        labels = [labels]\n",
        "\n",
        "                    for label in labels:\n",
        "                        entities.append((point['start'], point['end'] + 1, label))\n",
        "\n",
        "            cleaned_entities = clean_entities(text, entities)\n",
        "            training_data.append((text, {\"entities\": cleaned_entities}))\n",
        "\n",
        "        return training_data\n",
        "    except Exception as e:\n",
        "        logging.exception(f\"Unable to process {dataturks_JSON_FilePath}\\nerror = {str(e)}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "97OiQkJH952_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_spacy():\n",
        "    TRAIN_DATA = convert_dataturks_to_spacy(\"/content/traindata.json\")\n",
        "    nlp = spacy.load('en_core_web_lg')  # Load pre-trained model\n",
        "\n",
        "    if 'ner' not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe('ner')\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "    else:\n",
        "        ner = nlp.get_pipe('ner')\n",
        "\n",
        "    for _, annotations in TRAIN_DATA:\n",
        "        for ent in annotations.get('entities'):\n",
        "            ner.add_label(ent[2])\n",
        "\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    with nlp.disable_pipes(*other_pipes):\n",
        "        optimizer = nlp.resume_training()\n",
        "        best_loss = float('inf')\n",
        "        patience = 3  # Number of iterations to wait for improvement\n",
        "        no_improvement = 0\n",
        "\n",
        "        for itn in range(20):  # Start with a lower number of iterations\n",
        "            print(f\"Starting iteration {itn}\")\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            losses = {}\n",
        "            for batch in minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.5)):\n",
        "                texts, annotations = zip(*batch)\n",
        "                examples = [Example.from_dict(nlp.make_doc(text), annotation) for text, annotation in zip(texts, annotations)]\n",
        "                nlp.update(\n",
        "                    examples,\n",
        "                    drop=0.2,\n",
        "                    sgd=optimizer,\n",
        "                    losses=losses)\n",
        "            print(losses)\n",
        "\n",
        "            # Early stopping check\n",
        "            current_loss = losses.get('ner', float('inf'))\n",
        "            if current_loss < best_loss:\n",
        "                best_loss = current_loss\n",
        "                no_improvement = 0\n",
        "            else:\n",
        "                no_improvement += 1\n",
        "\n",
        "            if no_improvement >= patience:\n",
        "                print(f\"No improvement for {patience} iterations. Stopping training.\")\n",
        "                break\n",
        "\n",
        "    output_dir = \"/content/cv_parsing_model\"\n",
        "    nlp.to_disk(output_dir)\n",
        "    print(f\"Model saved to {output_dir}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GL8wl5eD-tkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_spacy()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3MUGP5XpB_8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "from pathlib import Path\n",
        "import random\n",
        "import string\n",
        "\n",
        "nlp = spacy.load(\"/content/drive/MyDrive/CV_Parser/cv_parsing_model\")\n",
        "\n",
        "# Get the NER component\n",
        "ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "# Add labels\n",
        "for _, annotations in TRAINING_DATA:\n",
        "        for ent in annotations.get('entities'):\n",
        "            ner.add_label(ent[2])\n",
        "\n",
        "# Convert the training data to spaCy examples\n",
        "examples = []\n",
        "for text, annotations in TRAINING_DATA:\n",
        "    doc = nlp.make_doc(text)\n",
        "    entites=annotations.get(\"entities\")\n",
        "    resolved_entities = []\n",
        "    seen_spans = set()\n",
        "    for start, end, label in entites:\n",
        "      # Check for overlaps and resolve them (keep the longest span)\n",
        "        if any((s, e) in seen_spans for s in range(start, end + 1) for e in range(start, end + 1)):\n",
        "            continue  # Skip this entity if it overlaps with an existing one\n",
        "        while start < end and (text[start].isspace() or text[start] in string.punctuation):\n",
        "            start += 1\n",
        "        while end > start and (text[end - 1].isspace() or text[end - 1] in string.punctuation):\n",
        "            end -= 1\n",
        "        if start < end:  # Only add the entity if it's still valid after trimming\n",
        "            resolved_entities.append((start, end, label))\n",
        "            seen_spans.update((s, e) for s in range(start, end + 1) for e in range(start, end + 1))  # Mark all character pairs within the span as seen\n",
        "    annotations[\"entities\"] = resolved_entities\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    examples.append(example)\n",
        "\n",
        "# Start the training\n",
        "optimizer = nlp.resume_training()\n",
        "move_names = list(ner.move_names)\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "\n",
        "best_loss = float('inf')\n",
        "no_improvement = 0\n",
        "patience = 3  # Number of iterations with no improvement to wait before stopping\n",
        "\n",
        "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "    sizes = compounding(1.0, 4.0, 1.001)\n",
        "    for itn in range(50):\n",
        "        random.shuffle(examples)\n",
        "        batches = minibatch(examples, size=sizes)\n",
        "        losses = {}\n",
        "        for batch in batches:\n",
        "            nlp.update(\n",
        "                batch,  # batch of Example objects\n",
        "                drop=0.2,  # dropout - make it harder to memorize data\n",
        "                losses=losses\n",
        "            )\n",
        "        print(f\"Iteration {itn}, Losses: {losses}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        current_loss = losses.get('ner', float('inf'))\n",
        "        if current_loss < best_loss:\n",
        "            best_loss = current_loss\n",
        "            no_improvement = 0\n",
        "        else:\n",
        "            no_improvement += 1\n",
        "\n",
        "        if no_improvement >= patience:\n",
        "            print(f\"No improvement for {patience} iterations. Stopping training.\")\n",
        "            break\n",
        "\n",
        "# Save the model to disk\n",
        "output_dir = Path(\"/content/drive/MyDrive/CV_Parser/cv_parsing_model_fineTuned2\")\n",
        "nlp.to_disk(output_dir)\n",
        "\n",
        "print(\"Training complete. Model saved to\", output_dir)"
      ],
      "metadata": {
        "id": "18RHzrtcYwPY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import pdfplumber\n",
        "import spacy\n",
        "import re\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to extract text from PDF using pdfplumber\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        text = ''\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Clean text\n",
        "    return text.strip()\n",
        "\n",
        "# Function to load and parse CV using custom spaCy model\n",
        "def parse_cv(cv_text, nlp_model):\n",
        "    doc = nlp_model(cv_text)\n",
        "    parsed_data = [(ent.label_, ent.text) for ent in doc.ents]\n",
        "    return parsed_data\n",
        "\n",
        "# Function to calculate cosine similarity between two vectors\n",
        "def calculate_cosine_similarity(vector1, vector2):\n",
        "    return cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))[0][0] * 100\n",
        "\n",
        "# Main function to process CV and job description\n",
        "def process_resume_and_job(cv_path, jd_path):\n",
        "    # Step 1: Extract text from PDFs\n",
        "    cv_text = extract_text_from_pdf(cv_path)\n",
        "    jd_text = extract_text_from_pdf(jd_path)\n",
        "\n",
        "    # Step 2: Preprocess text\n",
        "    cv_text = preprocess_text(cv_text)\n",
        "    jd_text = preprocess_text(jd_text)\n",
        "\n",
        "    # Step 3: Load custom spaCy model for CV parsing\n",
        "    nlp_cv = spacy.load(\"/content/drive/MyDrive/CV_Parser/cv_parsing_model_final/model-best\")\n",
        "\n",
        "    # Step 4: Parse CV using custom spaCy model\n",
        "    parsed_cv_data = parse_cv(cv_text, nlp_cv)\n",
        "    parsed_cv_text = ' '.join([text for _, text in parsed_cv_data]) if parsed_cv_data else ''\n",
        "\n",
        "    # Step 5: Generate embeddings using Gensim Doc2Vec\n",
        "    documents = [TaggedDocument(parsed_cv_text.split(), [0]), TaggedDocument(jd_text.split(), [1])]\n",
        "    model = Doc2Vec(documents, vector_size=100, window=5, min_count=1, workers=4)\n",
        "    cv_vector = model.dv[0]\n",
        "    jd_vector = model.dv[1]\n",
        "\n",
        "    # Step 6: Generate embeddings using Sentence Transformers\n",
        "    sentence_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "    cv_embeddings = sentence_model.encode([parsed_cv_text])\n",
        "    jd_embeddings = sentence_model.encode([jd_text])\n",
        "\n",
        "    # Step 7: Calculate cosine similarity scores\n",
        "    gensim_cosine_sim = calculate_cosine_similarity(cv_vector, jd_vector)\n",
        "    sentence_transformers_cosine_sim = calculate_cosine_similarity(cv_embeddings, jd_embeddings)\n",
        "\n",
        "    # Step 8: Final matching score\n",
        "    matching_score = (gensim_cosine_sim + sentence_transformers_cosine_sim) / 2\n",
        "\n",
        "    return matching_score\n"
      ],
      "metadata": {
        "id": "BXBaD4_WlfJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluating the model**"
      ],
      "metadata": {
        "id": "ODbXX3kjjoRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# Function to load the trained model\n",
        "def load_model(model_path):\n",
        "    nlp = spacy.load(model_path)\n",
        "    return nlp\n",
        "\n",
        "# Function to load test data\n",
        "def load_test_data(test_data_path):\n",
        "    import json\n",
        "    with open(test_data_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    examples = []\n",
        "    for entry in data:\n",
        "        text = entry['content']\n",
        "        annotations = entry['annotation']\n",
        "        entities = []\n",
        "        for annot in annotations:\n",
        "            label = annot['label'][0]\n",
        "            points = annot['points'][0]\n",
        "            start = points['start']\n",
        "            end = points['end']\n",
        "            entities.append((start, end, label))\n",
        "        examples.append((text, entities))\n",
        "    return examples\n",
        "\n",
        "# Function to test and evaluate the model\n",
        "def test_and_evaluate(nlp, test_data_path):\n",
        "    examples = load_test_data(test_data_path)\n",
        "    entity_stats = {}\n",
        "    overall_y_true = []\n",
        "    overall_y_pred = []\n",
        "\n",
        "    for text, annot in examples:\n",
        "        doc_to_test = nlp(text)\n",
        "\n",
        "        for ent in doc_to_test.ents:\n",
        "            entity_stats.setdefault(ent.label_, [0, 0, 0, 0, 0, 0])\n",
        "\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        for start, end, label in annot:\n",
        "            found = False\n",
        "            for ent in doc_to_test.ents:\n",
        "                if ent.start_char == start and ent.end_char == end and ent.label_ == label:\n",
        "                    y_true.append(ent.label_)\n",
        "                    y_pred.append(ent.label_)\n",
        "                    found = True\n",
        "                    break\n",
        "            if not found:\n",
        "                y_true.append(label)\n",
        "                y_pred.append('Not' + label)\n",
        "\n",
        "        overall_y_true.extend(y_true)\n",
        "        overall_y_pred.extend(y_pred)\n",
        "\n",
        "        for entity_type in entity_stats:\n",
        "            true_positive = 0\n",
        "            false_positive = 0\n",
        "            false_negative = 0\n",
        "            for i in range(len(y_true)):\n",
        "                if y_true[i] == entity_type and y_pred[i] == entity_type:\n",
        "                    true_positive += 1\n",
        "                elif y_true[i] == entity_type and y_pred[i] != entity_type:\n",
        "                    false_negative += 1\n",
        "                elif y_true[i] != entity_type and y_pred[i] == entity_type:\n",
        "                    false_positive += 1\n",
        "\n",
        "            if true_positive + false_positive > 0:\n",
        "                precision = true_positive / (true_positive + false_positive)\n",
        "            else:\n",
        "                precision = 0\n",
        "\n",
        "            if true_positive + false_negative > 0:\n",
        "                recall = true_positive / (true_positive + false_negative)\n",
        "            else:\n",
        "                recall = 0\n",
        "\n",
        "            if precision + recall > 0:\n",
        "                f1_score = 2 * precision * recall / (precision + recall)\n",
        "            else:\n",
        "                f1_score = 0\n",
        "\n",
        "            accuracy = (true_positive + (len(y_true) - (true_positive + false_positive + false_negative))) / len(y_true)\n",
        "\n",
        "            entity_stats[entity_type][0] = 1\n",
        "            entity_stats[entity_type][1] += precision\n",
        "            entity_stats[entity_type][2] += recall\n",
        "            entity_stats[entity_type][3] += f1_score\n",
        "            entity_stats[entity_type][4] += accuracy\n",
        "            entity_stats[entity_type][5] += 1\n",
        "\n",
        "    entity_accuracies = []\n",
        "    for entity_type, stats in entity_stats.items():\n",
        "        avg_accuracy = stats[4] / stats[5]\n",
        "        entity_accuracies.append(avg_accuracy)\n",
        "        print(f\"\\nFor Entity {entity_type}\\n\")\n",
        "        print(f\"Accuracy : {avg_accuracy * 100}%\")\n",
        "        print(f\"Precision : {stats[1] / stats[5]}\")\n",
        "        print(f\"Recall : {stats[2] / stats[5]}\")\n",
        "        print(f\"F-score : {stats[3] / stats[5]}\")\n",
        "\n",
        "    overall_accuracy = sum(entity_accuracies) / len(entity_accuracies)\n",
        "\n",
        "    print(\"\\nOverall Evaluation Report\\n\")\n",
        "    print(f\"Overall Accuracy: {overall_accuracy * 100}%\")\n",
        "\n",
        "# Example usage\n",
        "model_path = \"/content/drive/MyDrive/CV_Parser/cv_parsing_model_fineTuned2\"\n",
        "test_data_path = \"/content/drive/MyDrive/CV_Parser/testdata.json\"\n",
        "\n",
        "# Load the trained model\n",
        "nlp = load_model(model_path)\n",
        "\n",
        "# Test and evaluate the model\n",
        "test_and_evaluate(nlp, test_data_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTI_k-sPrdhx",
        "outputId": "8a3dc2f2-b548-4d64-dfc9-ff3052dc43bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "For Entity Name\n",
            "\n",
            "Accuracy : 94.1474471509705%\n",
            "Precision : 0.0\n",
            "Recall : 0.0\n",
            "F-score : 0.0\n",
            "\n",
            "For Entity Designation\n",
            "\n",
            "Accuracy : 88.81890860463179%\n",
            "Precision : 0.3\n",
            "Recall : 0.2666666666666667\n",
            "F-score : 0.275\n",
            "\n",
            "For Entity Companies worked at\n",
            "\n",
            "Accuracy : 88.54095781333129%\n",
            "Precision : 0.15\n",
            "Recall : 0.1125\n",
            "F-score : 0.12\n",
            "\n",
            "For Entity Location\n",
            "\n",
            "Accuracy : 87.09622674255195%\n",
            "Precision : 0.1\n",
            "Recall : 0.05\n",
            "F-score : 0.06666666666666667\n",
            "\n",
            "For Entity Email Address\n",
            "\n",
            "Accuracy : 95.04805194805193%\n",
            "Precision : 0.3\n",
            "Recall : 0.3\n",
            "F-score : 0.3\n",
            "\n",
            "For Entity Degree\n",
            "\n",
            "Accuracy : 93.29132185847087%\n",
            "Precision : 0.25\n",
            "Recall : 0.2\n",
            "F-score : 0.21666666666666665\n",
            "\n",
            "For Entity College Name\n",
            "\n",
            "Accuracy : 89.86533394683718%\n",
            "Precision : 0.3\n",
            "Recall : 0.16666666666666669\n",
            "F-score : 0.1983333333333333\n",
            "\n",
            "For Entity Skills\n",
            "\n",
            "Accuracy : 86.58471187899609%\n",
            "Precision : 0.1\n",
            "Recall : 0.05\n",
            "F-score : 0.06666666666666667\n",
            "\n",
            "For Entity Graduation Year\n",
            "\n",
            "Accuracy : 91.17645728286459%\n",
            "Precision : 0.05263157894736842\n",
            "Recall : 0.02631578947368421\n",
            "F-score : 0.03508771929824561\n",
            "\n",
            "For Entity Years of Experience\n",
            "\n",
            "Accuracy : 99.31481481481481%\n",
            "Precision : 0.0\n",
            "Recall : 0.0\n",
            "F-score : 0.0\n",
            "\n",
            "For Entity Can Relocate to\n",
            "\n",
            "Accuracy : 100.0%\n",
            "Precision : 0.0\n",
            "Recall : 0.0\n",
            "F-score : 0.0\n",
            "\n",
            "Overall Evaluation Report\n",
            "\n",
            "Overall Accuracy: 92.17129382195644%\n"
          ]
        }
      ]
    }
  ]
}